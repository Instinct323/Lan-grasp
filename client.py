from pathlib import Path
from typing import Callable

import api
from toolkit.client_utils import *

PROJECT = Path("runs")
PROJECT.mkdir(exist_ok=True)


class Gripper:
    """ :param obj: target object
        :param task: task name
        :param ngrid: number of grids for the object """
    fapi = FunctionsAPI(functions=api.FUNCTIONS)

    def __init__(self,
                 obj: str,
                 task: str,
                 ngrid: int = 30):
        self.obj = obj
        self.task = task
        self.grid_anno = GridAnnotator(ngrid)

        self.json_prompter = JSONprompter(
            grasping_part="A word or phrase describing the part to grasp.",
            grid_id="The grid ID of the part to grasp.",
            reason="A sentence explaining why this part is the best choice for grasping.",
        )
        self.prompt_template = (
                f"You are an intelligent robotic arm. "
                f"If you want to {self.task} the {self.obj} in the image, which part makes the most sense to grasp? Name one part. "
                f"Most tools will have a handle, and the handle is usually the best part to grasp. "
                f"The image has been divided into %d grids, "
                f"and the corresponding grid IDs are given by the array generated by `np.arange(%d).reshape(%s)`."
                + self.json_prompter.prompt()
        )

    def estimate_grasp(self,
                       bgr: np.ndarray,
                       depth: np.ndarray,
                       unproj: Callable):
        # Async: Obtain the grasping area
        mask_grasp = self.fapi.executor.submit(self.grasp_area, bgr)

        # Unproject the depth map
        pcd_full = unproj(depth)  # [H, W, 3]
        grasp_poses = self.fapi.invoke("ContactGraspNet", pcd_full.reshape(-1, 3))

        # Unproject the mask
        mask_grasp = mask_grasp.result()
        if mask_grasp is None: return
        pcd_mask = pcd_full[mask_grasp[:, 1], mask_grasp[:, 0]]  # [N, 3]
        # TODO: Outlier removal

        # TODO: Filter the grasping poses
        pcd_mask_bound = np.stack([pcd_mask.min(axis=0), pcd_mask.max(axis=0)], axis=0)

        return mask_grasp

    def grasp_area(self,
                   bgr: np.ndarray):
        """ :param bgr: observe image
            :return: grasping area """

        # Grouding the object
        dets_obj = self.fapi.invoke("GroundingDINO", bgr, caption=self.obj)
        if not dets_obj:
            LOGGER.error(f"Failed to detect \"{self.obj}\"")
            return
        LOGGER.info("GroudingDINO confidence: " + str(dets_obj.confidence))
        bbox_obj = np.round(dets_obj.xyxy[0]).astype(int)
        r = (bbox_obj[2:] - bbox_obj[:2]) / bgr.shape[1::-1]
        if max(r) > 0.98:
            LOGGER.warning("Too close to the object, skip grasping area estimation.")
            return
        bgr_obj = sv.crop_image(bgr, bbox_obj[None])

        # Async: Segment the object
        mask_obj = self.fapi.invoke_async("SAM2", bgr_obj, box=[[0, 0, *bgr_obj.shape[1::-1]]])

        # VLM reasoning
        grid_info = self.grid_anno.make_grid(*bgr_obj.shape[1::-1])
        bgr_obj = self.grid_anno.annotate(bgr_obj, grid_info)
        prompt = self.prompt_template % (grid_info["ngrid"], grid_info["ngrid"], grid_info["shape"])
        LOGGER.info("Prompt: " + prompt)
        vlm_ret = self.fapi.invoke("Qwen-VL", None, image=bgr_obj, text=prompt)
        vlm_ret = self.json_prompter.decode(vlm_ret)
        LOGGER.info("JSON response: " + str(vlm_ret))

        # Obtain the grasping area
        bbox_grasp = self.grid_anno.index_grid(vlm_ret.get("grid_id", -1), grid_info)
        if bbox_grasp is None:
            LOGGER.error("Invalid grid ID, please ensure the VLM is working correctly.")
            return
        mask_obj = mask_obj.result().mask[0]
        mask_grasp = sv.crop_image(mask_obj, bbox_grasp[None])
        mask_grasp = np.stack(np.where(mask_grasp)[::-1], axis=-1) + bbox_obj[:2] + bbox_grasp[:2]
        bbox_grasp = sv.move_boxes(bbox_grasp, bbox_obj[:2])

        # Visualize the results
        dets_grasp = np.zeros(bgr.shape[:2], dtype=np.bool_)
        dets_grasp[mask_grasp[:, 1], mask_grasp[:, 0]] = True
        dets_grasp = sv.Detections(xyxy=bbox_grasp[None], mask=dets_grasp[None])
        bgr = bgr.copy()
        bgr[bbox_obj[1]: bbox_obj[3], bbox_obj[0]: bbox_obj[2]] = bgr_obj
        bgr = sv_annotate(bgr, dets_grasp)
        cv2.imwrite(str(PROJECT / f"{self.task}-{self.obj}.jpg"), bgr)
        return mask_grasp


if __name__ == '__main__':
    gripper = Gripper("glass", "drink")
    # camera = Pinhole()

    img = cv2.imread("assets/glass.jpeg")
    img = sv.resize_image(img, [640] * 2, keep_aspect_ratio=True)

    output = gripper.grasp_area(img)
    print(output.shape)
